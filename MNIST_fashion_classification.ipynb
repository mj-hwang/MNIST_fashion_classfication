{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST fashion classification\n",
    "**This notebook is a demo of classifying MNIST fashion dataset.**\n",
    "\n",
    "1. importing CNN models (defined in `image_models.py`) and acquiring data.\n",
    "2. defining necessary functions, such as loss function or train function.\n",
    "3. comparing hyperparameters (weight decay, learning rate) and train methods.\n",
    "4. comparing different models with visualization.\n",
    "5. summarizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing CNN models and Acquiring data\n",
    "\n",
    "In this project, I used mxnet's ML module to define my CNN models. Please check implemetation of them in `image_models.py`. I have following pre-defined models.\n",
    "- Basic MLP (no convlutional layer)\n",
    "- LeNet\n",
    "- AlexNet\n",
    "- VGG 11\n",
    "- ResNet 18\n",
    "- ResNet 34\n",
    "\n",
    "I used FashionMNIST dataset (https://github.com/zalandoresearch/fashion-mnist), a popular image classification dataset for benchmarking machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "from image_models import *\n",
    "\n",
    "train = gdata.vision.FashionMNIST(train=True)\n",
    "test = gdata.vision.FashionMNIST(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining necessary functions.\n",
    "\n",
    "- loss function\n",
    "- train function\n",
    "- accuracy function\n",
    "- data loader & batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'transform_first'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8e5f38a33742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mgdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m train_iter = gdata.DataLoader(train.transform_first(transformer),\n\u001b[0m\u001b[0;32m     45\u001b[0m                               \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                               num_workers=num_workers)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'transform_first'"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# if you are using a machine with a GPU, set a context as GPU.\n",
    "# if you are using a machine without a GPU, uncomment the line .\n",
    "# context = mx.cpu()\n",
    "context = mx.gpu()\n",
    "\n",
    "# train fuction\n",
    "def train(net, train_iter, test_iter, batch_size, trainer, num_epochs, loss):\n",
    "    # iterate through epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            y = y.as_in_context(context)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X.as_in_context(context))\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d --> loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))\n",
    "\n",
    "# accuracy function\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = nd.array([0], ctx=context), 0\n",
    "    for X, y in data_iter:\n",
    "        X = X.as_in_context(context), \n",
    "        y = y.as_in_context(context).astype('float32')\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.size\n",
    "    return acc_sum.asscalar() / n\n",
    "\n",
    "# data loader, batch size, num_epochs\n",
    "batch_size = 256\n",
    "num_epochs = \n",
    "gdata.vision.transforms.ToTensor()\n",
    "train_iter = gdata.DataLoader(train.transform_first(transformer),\n",
    "                              batch_size, shuffle=True,\n",
    "                              num_workers=num_workers)\n",
    "test_iter = gdata.DataLoader(test.transform_first(transformer),\n",
    "                             batch_size, shuffle=False,\n",
    "                             num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing hyperparameters and train methods.\n",
    "\n",
    "We have following hyperpameters/learning methods to tune.\n",
    "\n",
    "- learning rate\n",
    "    - 1, 0.1, 0.01\n",
    "- trainer function\n",
    "    - SGD\n",
    "    - Adam\n",
    "- weight decay\n",
    "    - 0.3 ~ 0.5\n",
    "- batch normalization\n",
    "- activation layer\n",
    "    - Sigmoid\n",
    "    - ReLu (https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "- pooling type\n",
    "    - average pooling\n",
    "    - max pooling\n",
    "    \n",
    "**We will tune the hyperparameter with the base model of LeNet.**\n",
    "\n",
    "*for dropout, I set the proportion as 0.5 (as default).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1, 0.1, 0.01]\n",
    "for lr in lrs:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
