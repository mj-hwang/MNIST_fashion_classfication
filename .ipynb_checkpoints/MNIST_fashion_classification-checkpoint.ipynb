{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST fashion classification\n",
    "**This notebook is a demo of classifying MNIST fashion dataset.**\n",
    "\n",
    "1. importing CNN models (defined in `image_models.py`) and acquiring data.\n",
    "2. defining necessary functions, such as loss function or train function.\n",
    "3. comparing hyperparameters (weight decay, learning rate) and train methods.\n",
    "4. comparing different models with visualization.\n",
    "5. summarizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing CNN models and Acquiring data\n",
    "\n",
    "In this project, I used mxnet's ML module to define my CNN models. Please check implemetation of them in `image_models.py`. I have following pre-defined models.\n",
    "- Basic MLP (no convlutional layer)\n",
    "- LeNet\n",
    "- AlexNet\n",
    "- VGG 11\n",
    "- ResNet 18\n",
    "- ResNet 34\n",
    "\n",
    "I used FashionMNIST dataset (https://github.com/zalandoresearch/fashion-mnist), a popular image classification dataset for benchmarking machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "from image_models import *\n",
    "\n",
    "train_data = gdata.vision.FashionMNIST(train=True)\n",
    "test_data = gdata.vision.FashionMNIST(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining necessary functions.\n",
    "\n",
    "- loss function\n",
    "- train function\n",
    "- accuracy function\n",
    "- data loader & batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "# since we aim to classify images into different categories, we use softmax.\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# if you are using a machine with a GPU, set a context as GPU.\n",
    "# if you are using a machine without a GPU, uncomment the following line .\n",
    "# context = mx.cpu()\n",
    "context = mx.gpu()\n",
    "\n",
    "# train fuction\n",
    "def train(net, train_iter, test_iter, batch_size, trainer, num_epochs, loss):\n",
    "    # iterate through epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            y = y.as_in_context(context)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X.as_in_context(context))\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d --> loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))\n",
    "\n",
    "# accuracy function (that returns average accuracy)\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    # we have to use nd array for cumulative accuracy\n",
    "    cum_acc = nd.array([0], ctx=context)\n",
    "    cum_size = 0\n",
    "    for X, y in data_iter:\n",
    "        X = X.as_in_context(context) \n",
    "        y = y.as_in_context(context).astype('float32')\n",
    "        cum_acc += (net(X).argmax(axis=1) == y).sum()\n",
    "        cum_size += y.size\n",
    "    \n",
    "    # return the average accuracy as scalar\n",
    "    return cum_acc.asscalar() / cum_size\n",
    "\n",
    "# data loader & batch size\n",
    "batch_size = 128\n",
    "transformer = gdata.vision.transforms.ToTensor()\n",
    "train_iter = gdata.DataLoader(train_data.transform_first(transformer),\n",
    "                              batch_size, shuffle=True)\n",
    "test_iter = gdata.DataLoader(test_data.transform_first(transformer),\n",
    "                             batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing hyperparameters and train methods.\n",
    "\n",
    "We have following hyperpameters/learning methods to tune.\n",
    "\n",
    "- learning rate\n",
    "    - 1, 0.1, 0.01\n",
    "- trainer function\n",
    "    - SGD\n",
    "    - Adam (https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)\n",
    "- weight decay\n",
    "    - 0.3 ~ 0.5\n",
    "- batch normalization\n",
    "- activation layer\n",
    "    - Sigmoid\n",
    "    - ReLu (https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "- pooling type\n",
    "    - average pooling\n",
    "    - max pooling\n",
    "    \n",
    "**We will tune the hyperparameter with the base model of LeNet.**\n",
    "\n",
    "*`get_LeNet` function has following default parameters, as shown in `image_models.py`*\n",
    "- *pooling='avg'*\n",
    "- *activation='sigmoid*' \n",
    "- *batch_norm=False*\n",
    "\n",
    "\n",
    "*For dropout, I set the proportion as 0.5 (as default).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying lr: 1\n",
      "-------------------------\n",
      "epoch 1 --> loss 2.3129, train acc 0.100, test acc 0.100, time 6.2 sec\n",
      "epoch 2 --> loss 2.0652, train acc 0.196, test acc 0.583, time 5.9 sec\n",
      "epoch 3 --> loss 0.8443, train acc 0.664, test acc 0.714, time 6.1 sec\n",
      "epoch 4 --> loss 0.6299, train acc 0.754, test acc 0.772, time 5.8 sec\n",
      "epoch 5 --> loss 0.5433, train acc 0.791, test acc 0.791, time 5.8 sec\n",
      "epoch 6 --> loss 0.4899, train acc 0.813, test acc 0.821, time 5.8 sec\n",
      "epoch 7 --> loss 0.4520, train acc 0.830, test acc 0.840, time 5.9 sec\n",
      "epoch 8 --> loss 0.4235, train acc 0.842, test acc 0.856, time 5.9 sec\n",
      "epoch 9 --> loss 0.4021, train acc 0.850, test acc 0.859, time 5.9 sec\n",
      "epoch 10 --> loss 0.3811, train acc 0.858, test acc 0.864, time 6.0 sec\n",
      "-------------------------\n",
      "\n",
      "trying lr: 0.1\n",
      "-------------------------\n",
      "epoch 1 --> loss 2.3079, train acc 0.101, test acc 0.100, time 6.0 sec\n",
      "epoch 2 --> loss 2.3044, train acc 0.105, test acc 0.100, time 5.9 sec\n",
      "epoch 3 --> loss 2.2693, train acc 0.162, test acc 0.369, time 5.8 sec\n",
      "epoch 4 --> loss 1.5145, train acc 0.496, test acc 0.571, time 5.8 sec\n",
      "epoch 5 --> loss 1.0712, train acc 0.588, test acc 0.606, time 5.8 sec\n",
      "epoch 6 --> loss 0.9394, train acc 0.640, test acc 0.661, time 5.8 sec\n",
      "epoch 7 --> loss 0.8533, train acc 0.680, test acc 0.691, time 5.8 sec\n",
      "epoch 8 --> loss 0.7938, train acc 0.705, test acc 0.711, time 5.8 sec\n",
      "epoch 9 --> loss 0.7424, train acc 0.721, test acc 0.723, time 5.8 sec\n",
      "epoch 10 --> loss 0.7010, train acc 0.733, test acc 0.737, time 5.8 sec\n",
      "-------------------------\n",
      "\n",
      "trying lr: 0.01\n",
      "-------------------------\n",
      "epoch 1 --> loss 2.3108, train acc 0.102, test acc 0.100, time 5.8 sec\n",
      "epoch 2 --> loss 2.3030, train acc 0.098, test acc 0.103, time 5.8 sec\n",
      "epoch 3 --> loss 2.3030, train acc 0.100, test acc 0.100, time 5.8 sec\n",
      "epoch 4 --> loss 2.3028, train acc 0.100, test acc 0.100, time 5.8 sec\n",
      "epoch 5 --> loss 2.3028, train acc 0.100, test acc 0.100, time 5.8 sec\n",
      "epoch 6 --> loss 2.3026, train acc 0.103, test acc 0.100, time 5.8 sec\n",
      "epoch 7 --> loss 2.3027, train acc 0.101, test acc 0.100, time 5.9 sec\n",
      "epoch 8 --> loss 2.3026, train acc 0.100, test acc 0.100, time 5.9 sec\n",
      "epoch 9 --> loss 2.3025, train acc 0.102, test acc 0.100, time 5.9 sec\n",
      "epoch 10 --> loss 2.3023, train acc 0.103, test acc 0.100, time 5.8 sec\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrs = [1, 0.1, 0.01]\n",
    "for lr in lrs:\n",
    "    print(\"trying lr: {}\".format(lr))\n",
    "    print(\"-------------------------\")\n",
    "    # with default parameters \n",
    "    LeNet = get_LeNet()\n",
    "    num_epochs = 10\n",
    "    LeNet.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "    trainer = gluon.Trainer(LeNet.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    train(LeNet, train_iter, test_iter, batch_size, trainer, num_epochs, loss)\n",
    "    print(\"-------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems like we can use the learning rate of 1 or 0.1, but since we plan on training with more epochs later, we will use 0.1 as our learning rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Trainer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying trainer type:  sgd\n",
      "-------------------------\n",
      "epoch 1 --> loss 2.3105, train acc 0.099, test acc 0.100, time 5.8 sec\n",
      "epoch 2 --> loss 2.3074, train acc 0.100, test acc 0.100, time 5.8 sec\n",
      "epoch 3 --> loss 2.3020, train acc 0.111, test acc 0.170, time 5.8 sec\n",
      "epoch 4 --> loss 2.1054, train acc 0.277, test acc 0.514, time 5.8 sec\n",
      "epoch 5 --> loss 1.2565, train acc 0.548, test acc 0.585, time 5.9 sec\n",
      "epoch 6 --> loss 1.0079, train acc 0.611, test acc 0.654, time 5.9 sec\n",
      "epoch 7 --> loss 0.8955, train acc 0.666, test acc 0.686, time 5.8 sec\n",
      "epoch 8 --> loss 0.8332, train acc 0.691, test acc 0.683, time 5.8 sec\n",
      "epoch 9 --> loss 0.7834, train acc 0.711, test acc 0.721, time 6.0 sec\n",
      "epoch 10 --> loss 0.7340, train acc 0.727, test acc 0.726, time 6.7 sec\n",
      "-------------------------\n",
      "\n",
      "trying trainer type:  adam\n",
      "-------------------------\n",
      "epoch 1 --> loss 2.4372, train acc 0.100, test acc 0.100, time 6.8 sec\n",
      "epoch 2 --> loss 2.4034, train acc 0.098, test acc 0.100, time 6.8 sec\n",
      "epoch 3 --> loss 2.4053, train acc 0.101, test acc 0.100, time 6.8 sec\n",
      "epoch 4 --> loss 2.4020, train acc 0.098, test acc 0.100, time 6.7 sec\n",
      "epoch 5 --> loss 2.4067, train acc 0.101, test acc 0.100, time 6.7 sec\n",
      "epoch 6 --> loss 2.4103, train acc 0.099, test acc 0.100, time 6.8 sec\n",
      "epoch 7 --> loss 2.4030, train acc 0.100, test acc 0.100, time 6.9 sec\n",
      "epoch 8 --> loss 2.3995, train acc 0.100, test acc 0.100, time 6.7 sec\n",
      "epoch 9 --> loss 2.4120, train acc 0.098, test acc 0.100, time 6.8 sec\n",
      "epoch 10 --> loss 2.4000, train acc 0.101, test acc 0.100, time 6.9 sec\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trainer types\n",
    "t_types = ['sgd', 'adam']\n",
    "for t_type in t_types:\n",
    "    print(\"trying trainer type: \", t_type)\n",
    "    print(\"-------------------------\")\n",
    "    # with default parameters \n",
    "    LeNet = get_LeNet()\n",
    "    num_epochs = 10\n",
    "    LeNet.initialize(force_reinit=True, ctx=context, init=init.Xavier())\n",
    "    trainer = gluon.Trainer(LeNet.collect_params(), t_type, \n",
    "                            {'learning_rate': lr})\n",
    "    train(LeNet, train_iter, test_iter, batch_size, trainer, num_epochs, loss)\n",
    "    print(\"-------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use sgd as a trainer in this demo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing different models with visualization.\n",
    "\n",
    "We will compare following neural network models with hyperparameters from part 3.\n",
    "\n",
    "- Basic MLP (no convlutional layer)\n",
    "- LeNet\n",
    "- AlexNet\n",
    "- VGG 11\n",
    "- ResNet 18\n",
    "- ResNet 34\n",
    "\n",
    "I will import models from `image_models.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
